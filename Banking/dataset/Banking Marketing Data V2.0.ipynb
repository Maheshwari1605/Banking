{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline #Magic command to include plots in the notebook\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Banking Data Frame\n",
    "banking_df = pd.read_csv(\"../Data/bank-full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------Explorartory Data Analysis----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start EDA - Exploratory Data Analysis\n",
    "number_records = banking_df.shape[0]\n",
    "number_columns = banking_df.shape[1]\n",
    "\n",
    "print (\"Number of Records: \",number_records)\n",
    "print (\"Number of Columns: \",number_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banking_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data Type of Each Column\n",
    "print (banking_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "INSIGHT:\n",
    "--Multiple features are of string data type, so we will have to perform transformation into appropriate data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value Exploration\n",
    "print (banking_df.info()) #Count of Null Object\n",
    "\n",
    "# Converting string into categorical\n",
    "for feature in banking_df.columns: \n",
    "    if banking_df[feature].dtype == 'object': \n",
    "        banking_df[feature] = pd.Categorical(banking_df[feature])\n",
    "\n",
    "print (banking_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the Value Count\n",
    "print(banking_df.job.value_counts())\n",
    "print('\\n',banking_df.marital.value_counts())\n",
    "print('\\n',banking_df.education.value_counts())\n",
    "print('\\n',banking_df.default.value_counts())\n",
    "print('\\n',banking_df.housing.value_counts())\n",
    "print('\\n',banking_df.loan.value_counts())\n",
    "print('\\n',banking_df.contact.value_counts())\n",
    "print('\\n',banking_df.month.value_counts())\n",
    "print('\\n',banking_df.poutcome.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (banking_df.isnull().sum())\n",
    "print (banking_df.isnull().values.any())\n",
    "print (banking_df.isna().any())\n",
    "\n",
    "for column in banking_df.columns:\n",
    "    print (column,\": \",sum(banking_df[column] == \"none\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Descriptive Statistical Report \n",
    "banking_df_transpose = banking_df.describe().T\n",
    "print (banking_df_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT:\n",
    "    - Spread is Very High\n",
    "    - We might have outliers in the data\n",
    "    - We shoudnt go with mean as missing value replacement technique.\n",
    "    - Columns are on different scale, so we might have to perform scaling (Standarization/Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Outliers\n",
    "sns.boxplot(data=banking_df, orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banking_df.boxplot(return_type='axes',figsize=(30,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = []\n",
    "iqr_list = []\n",
    "out_low = []\n",
    "out_up = []\n",
    "tot_ou = []\n",
    "for column in banking_df.describe().columns:\n",
    "    QTR1 = banking_df.describe().at['25%', column]\n",
    "    QTR3 = banking_df.describe().at['75%', column]\n",
    "    IQR = QTR3-QTR1\n",
    "    LTV = QTR1 - 1.5 * IQR # lower bound \n",
    "    UTV = QTR3 + 1.5 * IQR # upper bound\n",
    "    current_column = column\n",
    "    current_iqr = IQR\n",
    "    outliers_bl_low_bount = banking_df[banking_df[column] < LTV][column].count()\n",
    "    outliers_bl_up_bount = banking_df[banking_df[column] > UTV][column].count()\n",
    "    total_num_of_outliers = outliers_bl_low_bount + outliers_bl_up_bount\n",
    "    \n",
    "    column_list.append(current_column)\n",
    "    iqr_list.append(current_iqr)\n",
    "    out_low.append(outliers_bl_low_bount)\n",
    "    out_up.append(outliers_bl_up_bount)\n",
    "    tot_ou.append(total_num_of_outliers)\n",
    "\n",
    "outlier_report = {\"Column Name\":column_list,\"IQR\":iqr_list,\"Below Outliers\":out_low,\"Above Outliers\":out_up,\"Total No Of Outliers\":tot_ou}\n",
    "outlier_report = pd.DataFrame(outlier_report)\n",
    "\n",
    "print (outlier_report)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"----------------------------------------Visualization-------------------------------------------------------------\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(banking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (banking_df.Target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of Age on Target\n",
    "fig,ax1 = plt.subplots()\n",
    "\n",
    "#Age\n",
    "bins = range(0,100,10)\n",
    "sns.distplot(banking_df.age[banking_df.Target=='yes'],color='r',bins=bins,label=\"Subscribed\",ax=ax1,kde=False)\n",
    "sns.distplot(banking_df.age[banking_df.Target=='no'],color='b',bins=bins,label=\"Not Subscribed\",ax=ax1,kde=False)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: Age might be one important parameter, especially in range of 20-60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impact of Jobs on Target\n",
    "fig,ax2 = plt.subplots()\n",
    "sns.countplot(banking_df['job'], data = banking_df, hue = 'Target', ax = ax2)\n",
    "sns.despine(ax = ax2)\n",
    "ax2.set_xlabel('Job', fontsize=5)\n",
    "ax2.set_ylabel('Occurence', fontsize=5)\n",
    "ax2.set_title('Job x Ocucurence', fontsize=5)\n",
    "ax2.tick_params(labelsize=15)\n",
    "ax2.set_xticklabels(banking_df['job'], rotation=90)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.tight_layout() \n",
    "plt.legend(title=\"Subscribers\",labels=[\"Not Subscribed\",\"Subscribed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: Few profiles are helpful for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------Start The Modellig Process----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_report_tracker = {\"Algo Version\":[],\"Precision_Yes\":[],\"Precision_No\":[],\"F1_Yes\":[],\"F1_No\":[],\"Recall_Yes\":[],\"Recall_No\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (model_report_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age           job  marital  education default  balance housing loan  \\\n",
      "0   58    management  married   tertiary      no     2143     yes   no   \n",
      "1   44    technician   single  secondary      no       29     yes   no   \n",
      "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
      "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
      "4   33       unknown   single    unknown      no        1      no   no   \n",
      "\n",
      "   contact Target  \n",
      "0  unknown     no  \n",
      "1  unknown     no  \n",
      "2  unknown     no  \n",
      "3  unknown     no  \n",
      "4  unknown     no  \n"
     ]
    }
   ],
   "source": [
    "banking_sub_df = banking_df.iloc[:,[0,1,2,3,4,5,6,7,8,16]]\n",
    "print (banking_sub_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45211, 31)\n",
      "Index(['age', 'balance', 'Target', 'job_admin.', 'job_blue-collar',\n",
      "       'job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired',\n",
      "       'job_self-employed', 'job_services', 'job_student', 'job_technician',\n",
      "       'job_unemployed', 'job_unknown', 'marital_divorced', 'marital_married',\n",
      "       'marital_single', 'education_primary', 'education_secondary',\n",
      "       'education_tertiary', 'education_unknown', 'default_no', 'default_yes',\n",
      "       'housing_no', 'housing_yes', 'loan_no', 'loan_yes', 'contact_cellular',\n",
      "       'contact_telephone', 'contact_unknown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Dummy Variable Creation\n",
    "categorical_column = ['job','marital','education','default','housing','loan','contact']\n",
    "banking_sub_df = pd.get_dummies(banking_sub_df,columns=categorical_column)\n",
    "\n",
    "print (banking_sub_df.shape)\n",
    "print (banking_sub_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================Class Imbalance Treatment ====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Down Sampling: \n",
    "    -- Reducing number of rows for majority class\n",
    "\"\"\"\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "banking_df_majority = banking_sub_df[banking_sub_df.Target == \"no\"]\n",
    "banking_df_minority = banking_sub_df[banking_sub_df.Target == \"yes\"]\n",
    "\n",
    "# Downsampling Majority Class\n",
    "# Downsample majority class\n",
    "majority_down = resample(banking_df_majority, replace=False,\n",
    "                                   n_samples=7000, random_state=123)\n",
    "banking_down_sample_df = pd.concat([majority_down,banking_df_minority])\n",
    "\n",
    "print (banking_down_sample_df.Target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block for undersampling & raw distribution\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X = banking_down_sample_df.drop('Target',axis=1) #Input Data Set\n",
    "Y = banking_down_sample_df[[\"Target\"]] #Label or Outcome Column\n",
    "\n",
    "# X = banking_sub_df.drop('Target',axis=1) #Input Data Set\n",
    "# Y = banking_sub_df[[\"Target\"]] #Label or Outcome Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes    39922\n",
      "no     39922\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Code for Oversampling\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "predictors = banking_df.iloc[:,0:16]\n",
    "predictors = predictors.drop(['pdays'],axis=1)\n",
    "Y = banking_df.iloc[:,16]\n",
    "X = pd.get_dummies(predictors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Random Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "# over_sampler = RandomOverSampler(random_state=0)\n",
    "# x_over, y_over = over_sampler.fit_resample(X, Y)\n",
    "\n",
    "# X = x_over\n",
    "# Y = y_over\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "x_smote, y_smote = smote.fit_resample(X, Y)\n",
    "# pd.Series(y_SMOTE).value_counts()\n",
    "\n",
    "print (pd.Series(y_smote).value_counts())\n",
    "\n",
    "X = x_smote\n",
    "Y = y_smote\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=7)\n",
    "# print (y_train.Target.value_counts())\n",
    "# print (y_test.Target.value_counts())\n",
    "# print('x train data: ',x_train.shape)\n",
    "# print('y train data:',y_train.shape)\n",
    "# print('x test data : ',x_test.shape)\n",
    "# print('y test data :',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pd.Series(y_Osampled).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets apply scaling (Standarization or Normalization)\n",
    "x_train_scaled = preprocessing.scale(x_train)\n",
    "x_test_scaled = preprocessing.scale(x_test)\n",
    "\n",
    "x_train = x_train_scaled\n",
    "x_test  = x_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.35079058 -0.43600533 -1.30611109 ... -0.15581397 -0.19992672\n",
      "   0.73318721]\n",
      " [-1.35172555 -0.40018505  0.73060211 ... -0.15581397 -0.19992672\n",
      "   0.73318721]\n",
      " [-0.35079058 -0.00736606  0.60330753 ... -0.15581397 -0.19992672\n",
      "   0.73318721]\n",
      " ...\n",
      " [ 0.01318577 -0.4239649  -0.66963822 ... -0.15581397  5.00183266\n",
      "  -1.36390813]\n",
      " [-0.53277875 -0.47363167 -0.79693279 ... -0.15581397 -0.19992672\n",
      "  -1.36390813]\n",
      " [-0.71476693 -0.26473023 -0.79693279 ... -0.15581397 -0.19992672\n",
      "   0.73318721]]\n",
      "[[ 0.37723135  0.0308381  -0.03590778 ... -0.1615163  -0.20449668\n",
      "   0.74370862]\n",
      " [-0.79864361 -0.5116049  -1.18298538 ... -0.1615163  -0.20449668\n",
      "  -1.34461263]\n",
      " [-0.61773977 -0.43814183  2.00334128 ... -0.1615163  -0.20449668\n",
      "   0.74370862]\n",
      " ...\n",
      " [ 0.55813519  0.94860432 -0.29081391 ... -0.1615163  -0.20449668\n",
      "   0.74370862]\n",
      " [ 3.81440431 -0.50707874  1.23862288 ... -0.1615163  -0.20449668\n",
      "   0.74370862]\n",
      " [-1.25090321 -0.41585919  0.85626368 ... -0.1615163  -0.20449668\n",
      "  -1.34461263]]\n"
     ]
    }
   ],
   "source": [
    "print (x_train)\n",
    "print (x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------Logistic Regression-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94160927 0.94693175 0.94646212 0.94082655 0.93988729 0.94880225\n",
      " 0.94003444 0.94066072 0.94770628 0.94817598]\n",
      "{'no': {'precision': 0.9141575156452946, 'recall': 0.9767852636891244, 'f1-score': 0.9444342787435193, 'support': 7926}, 'yes': {'precision': 0.9754666666666667, 'recall': 0.9096108417257243, 'f1-score': 0.9413884063565592, 'support': 8043}, 'accuracy': 0.9429519694407915, 'macro avg': {'precision': 0.9448120911559806, 'recall': 0.9431980527074244, 'f1-score': 0.9429113425500393, 'support': 15969}, 'weighted avg': {'precision': 0.9450366878955856, 'recall': 0.9429519694407915, 'f1-score': 0.9429001844603255, 'support': 15969}}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.91      0.98      0.94      7926\n",
      "         yes       0.98      0.91      0.94      8043\n",
      "\n",
      "    accuracy                           0.94     15969\n",
      "   macro avg       0.94      0.94      0.94     15969\n",
      "weighted avg       0.95      0.94      0.94     15969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn import preprocessing\n",
    "#Prepare for cross validation\n",
    "seed = 10\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "LogReg = LogisticRegression(solver = 'lbfgs')\n",
    "LogReg.fit(x_train, y_train)\n",
    "\n",
    "# Predicting for test set\n",
    "y_pred               = LogReg.predict(x_test)\n",
    "# train_accuracy = accuracy_score(y_train,train_pred)\n",
    "# test_accuracy = accuracy_score(y_test,y_pred)\n",
    "\n",
    "# print (train_accuracy,test_accuracy)\n",
    "\n",
    "cross_validation_result = model_selection.cross_val_score(LogReg, x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "print(cross_validation_result)\n",
    "\n",
    "cls_report = classification_report(y_test, y_pred,output_dict=True)\n",
    "print (cls_report)\n",
    "\n",
    "print (classification_report(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_report(model_report_tracker,cls_report,algo_version):\n",
    "    print (model_report_tracker)\n",
    "    no_precision = cls_report[\"no\"][\"precision\"]\n",
    "    no_recall = cls_report[\"no\"][\"recall\"]\n",
    "    no_f1 = cls_report[\"no\"][\"f1-score\"]\n",
    "\n",
    "    yes_precision = cls_report[\"yes\"][\"precision\"]\n",
    "    yes_recall = cls_report[\"yes\"][\"recall\"]\n",
    "    yes_f1 = cls_report[\"yes\"][\"f1-score\"]\n",
    "\n",
    "    # model_report_tracker = {\"Algo Version\":[],\"Precision_Yes\":[],\"Precision_No\":[],\"F1_Yes\":[],\"F1_No\":[]}\n",
    "\n",
    "    current_algo = model_report_tracker[\"Algo Version\"]\n",
    "    current_p_yes = model_report_tracker[\"Precision_Yes\"]\n",
    "    current_p_no = model_report_tracker[\"Precision_No\"]\n",
    "    current_r_yes = model_report_tracker[\"Recall_Yes\"]\n",
    "    current_r_no = model_report_tracker[\"Recall_No\"]\n",
    "    current_f_yes  = model_report_tracker[\"F1_Yes\"]\n",
    "    current_f_no = model_report_tracker[\"F1_No\"]\n",
    "    \n",
    "    print (current_algo)\n",
    "    print (current_p_yes)\n",
    "    print (current_p_no)\n",
    "\n",
    "    model_report_tracker[\"Algo Version\"] = current_algo.append(algo_version)\n",
    "    model_report_tracker[\"Precision_Yes\"] = current_p_yes.append(yes_precision)\n",
    "    model_report_tracker[\"Precision_No\"] = current_p_no.append(no_precision)\n",
    "    model_report_tracker[\"Recall_Yes\"] = current_r_yes.append(yes_recall)\n",
    "    model_report_tracker[\"Recall_No\"] = current_r_no.append(no_recall)\n",
    "    model_report_tracker[\"F1_Yes\"] = current_f_yes.append(yes_f1)\n",
    "    model_report_tracker[\"F1_No\"] = current_f_no.append(no_f1)\n",
    "    \n",
    "    return model_report_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (model_report_tracker)\n",
    "# report = insert_report(model_report_tracker,cls_report,\"Logistic Raw Data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================Decision Tree==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92454602 0.92924233 0.92924233 0.93409518 0.92579837 0.9304838\n",
      " 0.93126664 0.93111007 0.93220604 0.93251918]\n",
      "{'no': {'precision': 0.9259259259259259, 'recall': 0.9336361342417361, 'f1-score': 0.9297650458600327, 'support': 7926}, 'yes': {'precision': 0.9340604237181898, 'recall': 0.9263956235235609, 'f1-score': 0.9302122347066167, 'support': 8043}, 'accuracy': 0.9299893543740998, 'macro avg': {'precision': 0.9299931748220578, 'recall': 0.9300158788826485, 'f1-score': 0.9299886402833247, 'support': 15969}, 'weighted avg': {'precision': 0.9300229743161306, 'recall': 0.9299893543740998, 'f1-score': 0.9299902784915735, 'support': 15969}}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.93      0.93      0.93      7926\n",
      "         yes       0.93      0.93      0.93      8043\n",
      "\n",
      "    accuracy                           0.93     15969\n",
      "   macro avg       0.93      0.93      0.93     15969\n",
      "weighted avg       0.93      0.93      0.93     15969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dTree = DecisionTreeClassifier(criterion = 'entropy', random_state=1)\n",
    "dTree.fit(x_train, y_train)\n",
    "\n",
    "# Predicting for test set\n",
    "y_pred               = dTree.predict(x_test)\n",
    "\n",
    "# dTree_ScoreAccuracy        = accuracy_score(y_test, dTree_y_pred)\n",
    "# dTree_PrecisonScore        = precision_score(y_test, dTree_y_pred)\n",
    "# dTree_RecollScore          = recall_score(y_test, dTree_y_pred)\n",
    "# dTree_F1                   = f1_score(y_test, dTree_y_pred)\n",
    "\n",
    "cross_validation_result = model_selection.cross_val_score(dTree, x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "print (cross_validation_result)\n",
    "# dTree_models_results = pd.DataFrame([['Decision Tree ', dTree_ScoreAccuracy, dTree_PrecisonScore,\n",
    "#                                 dTree_RecollScore, dTree_F1, cross_validation_result.mean(), cross_validation_result.std()]], \n",
    "#                               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Mean', 'Std Deviation'])\n",
    "# base_model_results = base_model_results.append(dTree_models_results, ignore_index = True)\n",
    "# print(dTree.score(X_train, y_train))\n",
    "# print(dTree.score(X_test, y_test))\n",
    "\n",
    "print (classification_report(y_test, y_pred,output_dict = True))\n",
    "print (classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9456794  0.94802755 0.94787101 0.94630557 0.94395742 0.95115078\n",
      " 0.94598403 0.94488805 0.95209018 0.95287302]\n",
      "{'no': {'precision': 0.9259259259259259, 'recall': 0.9336361342417361, 'f1-score': 0.9297650458600327, 'support': 7926}, 'yes': {'precision': 0.9340604237181898, 'recall': 0.9263956235235609, 'f1-score': 0.9302122347066167, 'support': 8043}, 'accuracy': 0.9299893543740998, 'macro avg': {'precision': 0.9299931748220578, 'recall': 0.9300158788826485, 'f1-score': 0.9299886402833247, 'support': 15969}, 'weighted avg': {'precision': 0.9300229743161306, 'recall': 0.9299893543740998, 'f1-score': 0.9299902784915735, 'support': 15969}}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.93      0.93      0.93      7926\n",
      "         yes       0.93      0.93      0.93      8043\n",
      "\n",
      "    accuracy                           0.93     15969\n",
      "   macro avg       0.93      0.93      0.93     15969\n",
      "weighted avg       0.93      0.93      0.93     15969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomForest = RandomForestClassifier(n_estimators = 50, random_state=1, max_features=12)\n",
    "randomForest.fit(x_train, y_train)\n",
    "\n",
    "# Predicting for test set\n",
    "randomForest_y_pred               = randomForest.predict(x_test)\n",
    "\n",
    "rdf_cross_validation_result = model_selection.cross_val_score(randomForest, x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "print (rdf_cross_validation_result)\n",
    "\n",
    "# randomForest_results = pd.DataFrame([['Random Forest', randomForest_ScoreAccuracy, randomForest_PrecisonScore,\n",
    "#                                 randomForest_RecollScore, randomForest_F1, rdf_cross_validation_result.mean(), rdf_cross_validation_result.std()]], \n",
    "#                               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Mean', 'Std Deviation'])\n",
    "# print (randomForest_results)\n",
    "# ensemble_results = ensemble_results.append(randomForest_results, ignore_index = True)\n",
    "\n",
    "print (classification_report(y_test, y_pred,output_dict = True))\n",
    "print (classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
