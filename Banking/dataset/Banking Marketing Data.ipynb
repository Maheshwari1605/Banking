{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline #Magic command to include plots in the notebook\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Banking Data Frame\n",
    "banking_df = pd.read_csv(\"../Data/bank-full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------Explorartory Data Analysis----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start EDA - Exploratory Data Analysis\n",
    "number_records = banking_df.shape[0]\n",
    "number_columns = banking_df.shape[1]\n",
    "\n",
    "print (\"Number of Records: \",number_records)\n",
    "print (\"Number of Columns: \",number_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banking_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data Type of Each Column\n",
    "print (banking_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "INSIGHT:\n",
    "--Multiple features are of string data type, so we will have to perform transformation into appropriate data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value Exploration\n",
    "print (banking_df.info()) #Count of Null Object\n",
    "\n",
    "# Converting string into categorical\n",
    "for feature in banking_df.columns: \n",
    "    if banking_df[feature].dtype == 'object': \n",
    "        banking_df[feature] = pd.Categorical(banking_df[feature])\n",
    "\n",
    "print (banking_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the Value Count\n",
    "print(banking_df.job.value_counts())\n",
    "print('\\n',banking_df.marital.value_counts())\n",
    "print('\\n',banking_df.education.value_counts())\n",
    "print('\\n',banking_df.default.value_counts())\n",
    "print('\\n',banking_df.housing.value_counts())\n",
    "print('\\n',banking_df.loan.value_counts())\n",
    "print('\\n',banking_df.contact.value_counts())\n",
    "print('\\n',banking_df.month.value_counts())\n",
    "print('\\n',banking_df.poutcome.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (banking_df.isnull().sum())\n",
    "print (banking_df.isnull().values.any())\n",
    "print (banking_df.isna().any())\n",
    "\n",
    "for column in banking_df.columns:\n",
    "    print (column,\": \",sum(banking_df[column] == \"none\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Descriptive Statistical Report \n",
    "banking_df_transpose = banking_df.describe().T\n",
    "print (banking_df_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT:\n",
    "    - Spread is Very High\n",
    "    - We might have outliers in the data\n",
    "    - We shoudnt go with mean as missing value replacement technique.\n",
    "    - Columns are on different scale, so we might have to perform scaling (Standarization/Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Outliers\n",
    "sns.boxplot(data=banking_df, orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banking_df.boxplot(return_type='axes',figsize=(30,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = []\n",
    "iqr_list = []\n",
    "out_low = []\n",
    "out_up = []\n",
    "tot_ou = []\n",
    "for column in banking_df.describe().columns:\n",
    "    QTR1 = banking_df.describe().at['25%', column]\n",
    "    QTR3 = banking_df.describe().at['75%', column]\n",
    "    IQR = QTR3-QTR1\n",
    "    LTV = QTR1 - 1.5 * IQR # lower bound \n",
    "    UTV = QTR3 + 1.5 * IQR # upper bound\n",
    "    current_column = column\n",
    "    current_iqr = IQR\n",
    "    outliers_bl_low_bount = banking_df[banking_df[column] < LTV][column].count()\n",
    "    outliers_bl_up_bount = banking_df[banking_df[column] > UTV][column].count()\n",
    "    total_num_of_outliers = outliers_bl_low_bount + outliers_bl_up_bount\n",
    "    \n",
    "    column_list.append(current_column)\n",
    "    iqr_list.append(current_iqr)\n",
    "    out_low.append(outliers_bl_low_bount)\n",
    "    out_up.append(outliers_bl_up_bount)\n",
    "    tot_ou.append(total_num_of_outliers)\n",
    "\n",
    "outlier_report = {\"Column Name\":column_list,\"IQR\":iqr_list,\"Below Outliers\":out_low,\"Above Outliers\":out_up,\"Total No Of Outliers\":tot_ou}\n",
    "outlier_report = pd.DataFrame(outlier_report)\n",
    "\n",
    "print (outlier_report)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"----------------------------------------Visualization-------------------------------------------------------------\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(banking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (banking_df.Target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of Age on Target\n",
    "fig,ax1 = plt.subplots()\n",
    "\n",
    "#Age\n",
    "bins = range(0,100,10)\n",
    "sns.distplot(banking_df.age[banking_df.Target=='yes'],color='r',bins=bins,label=\"Subscribed\",ax=ax1,kde=False)\n",
    "sns.distplot(banking_df.age[banking_df.Target=='no'],color='b',bins=bins,label=\"Not Subscribed\",ax=ax1,kde=False)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: Age might be one important parameter, especially in range of 20-60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impact of Jobs on Target\n",
    "fig,ax2 = plt.subplots()\n",
    "sns.countplot(banking_df['job'], data = banking_df, hue = 'Target', ax = ax2)\n",
    "sns.despine(ax = ax2)\n",
    "ax2.set_xlabel('Job', fontsize=5)\n",
    "ax2.set_ylabel('Occurence', fontsize=5)\n",
    "ax2.set_title('Job x Ocucurence', fontsize=5)\n",
    "ax2.tick_params(labelsize=15)\n",
    "ax2.set_xticklabels(banking_df['job'], rotation=90)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.tight_layout() \n",
    "plt.legend(title=\"Subscribers\",labels=[\"Not Subscribed\",\"Subscribed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: Few profiles are helpful for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------Start The Modellig Process----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banking_sub_df = banking_df.iloc[:,[0,1,2,3,4,5,6,7,8,16]]\n",
    "print (banking_sub_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Variable Creation\n",
    "categorical_column = ['job','marital','education','default','housing','loan','contact']\n",
    "banking_sub_df = pd.get_dummies(banking_sub_df,columns=categorical_column)\n",
    "\n",
    "print (banking_sub_df.shape)\n",
    "print (banking_sub_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "X = banking_sub_df.drop('Target',axis=1) #Input Data Set\n",
    "Y = banking_sub_df[[\"Target\"]] #Label or Outcome Column\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=7)\n",
    "print (y_train.Target.value_counts())\n",
    "print (y_test.Target.value_counts())\n",
    "print('x train data: ',x_train.shape)\n",
    "print('y train data:',y_train.shape)\n",
    "print('x test data : ',x_test.shape)\n",
    "print('y test data :',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets apply scaling (Standarization or Normalization)\n",
    "x_train_scaled = preprocessing.scale(x_train)\n",
    "x_test_scaled = preprocessing.scale(x_test)\n",
    "\n",
    "x_train = x_train_scaled\n",
    "x_test  = x_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (x_train)\n",
    "print (x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8875373216852814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Byom Kesh Jha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88692286 0.86895217 0.88471109 0.87918164 0.87973459 0.88388167\n",
      " 0.88028753 0.88277578 0.88855088 0.88274336]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.89      1.00      0.94      8027\n",
      "         yes       0.33      0.00      0.00      1016\n",
      "\n",
      "    accuracy                           0.89      9043\n",
      "   macro avg       0.61      0.50      0.47      9043\n",
      "weighted avg       0.83      0.89      0.83      9043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn import preprocessing\n",
    "#Prepare for cross validation\n",
    "seed = 10\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "LogReg = LogisticRegression(solver = 'lbfgs')\n",
    "LogReg.fit(x_train, y_train)\n",
    "\n",
    "# Predicting for test set\n",
    "LogReg_y_pred               = LogReg.predict(x_test)\n",
    "LogReg_Score                = LogReg.score(x_test, y_test)\n",
    "print (LogReg_Score)\n",
    "\n",
    "# LogReg_ScoreAccuracy        = accuracy_score(y_test, LogReg_y_pred)\n",
    "\n",
    "# LogReg_PrecisonScore        = precision_score(y_test, LogReg_y_pred)\n",
    "# LogReg_RecollScore          = recall_score(y_test, LogReg_y_pred)\n",
    "# LogReg_F1                   = f1_score(y_test, LogReg_y_pred)\n",
    "\n",
    "cross_validation_result = model_selection.cross_val_score(LogReg, x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "print(cross_validation_result)\n",
    "\n",
    "# base_model_results = pd.DataFrame([['Logistic Regression', LogReg_ScoreAccuracy, LogReg_PrecisonScore,\n",
    "#                                 LogReg_RecollScore, LogReg_F1, cross_validation_result.mean(), cross_validation_result.std()]], \n",
    "#                               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Mean', 'Std Deviation'])\n",
    "\n",
    "print(classification_report(y_test, LogReg_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
